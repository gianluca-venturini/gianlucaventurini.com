---
title: TODO
snippet: TODO
cover: /uploads/blog-subscribe/blog-subscribe-cover.webp
date: 2025-02-02T15:00:00.000Z
---
<intro>

# AI-assisted development of complex systems
In the past year I've spent some time integrating LLMs in coding tasks and I wanted to share few thoughts. I'm mainly using various models to speed up coding tasks: function and framework API lookups and multi-file refactors. Whenever I'm working on a coding task, I often question whether I should personally write every piece of code or if there are simpler sections I can delegate and then review.

Initially my main use-case was speeding up typing speed of very easy code I could have easily written myself: let the model write something somewhat good, refine it manually and write few tests. I don't notice a significant practical difference between different models like Claude Sonnet 3.5 and GPT4o in this use-case. Recently.

I'm increasingly finding myself discussing different ideas with the chat during initial problem exploration. I think it's important that the tool used provides the model context about the current code selection and related files. Reasoning models like o1 or o3 help me understand the tradeoff and often point out something I don't know, and could take the time to learn. In the next section I'll talk about how to effectively learn.

## Great for learning
I noticed that when learning a new language or a new framework it's very useful to learn alongside an LLM. The inline code completion is useful to know about framework APIs and patterns I could be using. I frequently ask for the best pattern to solve a familiar problem in a new language or framework. For example, I recently wrote a Raft implementation in Rust and I was able to use concurrency primitives and futures without reading anything about them and just writing code that the model refined and made it compile.

Learning by doing assisted by an LLMs works really well for me. Learning a new pattern while I need it for solving a problem is way more effective than reading an entire blog about it or even worse the documentation. I often have a half-baked idea and have the model tell me how the average developer would do it in that specific language. E.g. how to spin up a series of threads and send parallel network requests, waiting at the end.

## Bad at strongly opinionated tasks
I find LLM pretty bad at holding strong opinions or pushing back on bad questions. When I'm exploring new problems or libraries I need to learn the terminology in order to ask detailed questions. I frequently find the answer too specific to the question. Rather than trying to generalize and helping me refine the question first.

<example generic question with too specific answer>

I don't know if this trait is desired and voluntarily baked-in during the final fine-tuning phase of the model training. I'm having hard time finding a model that pushes back on ill posed or incomplete questions, I find them all too condescending.

This is particularly bad when I'm trying to design a new architecture. For example while researching and prototyping modern solutions for frontend state management it's easy to have the model list all the libraries and simple examples. It is much harder to let it integrate them a simple module that contains a lot of custom patterns it hasn't seen before evolved over time in a legacy code-base.

## Bad for intellectual stimulation
I think LLMs lower the intellectual stimulation of a task. I'm sometimes finding myself delegating tasks that are mildly uncomfortable, like writing a complex boolean condition or calculating the index of the element of an array. This happens because many of the tools I'm using are auto-completing code for me.

Removing these small but frequent intellectual mini-puzzles decreases the satisfaction of a coding task, making it almost a chore, a box to check. The other more subtle problem is that it often doesn't implement complex code correctly, and since we didn't feel like writing it, we probably don't feel like thinking too hard about it either. It regularly misses corner cases and commits for example off-by-1 errors.

Having complex parts of the code written by the model and only half-checked is basically introducing black boxes and I believe it's a serious problem. There are a few mitigations. Firstly we could pay very close attention to what the model spits out and potentially rewrite the most complex or confusing parts. Additionally, we can manually (not the model, no cheating here!) write very comprehensive tests.

Let's talk about a couple of tools I'm using: Github CoPilot and Cursor. Note that these tools are improving really fast and likely the next section will be outdated soon.

# Github CoPilot
It is a VSCode extension. I find it a little clunky to use: it's pretty slow in generating code, especially if the module is large. It requires to add all the files we want to edit manually and doesn't have good discovering capabilities. The model selection is also a little limited, but as mentioned before I don't notice a huge difference between them, so not a deal breaker.

I think it's a good initial step into LLM-collaboration code because it has the basic features of chat and compose code in a small amount of files.

<example find redis client>

# Cursor Editor
The Cursor team decided for a different approach: they forked VSCode and built many features that are not available simply writing an extension. 

## What I like
Discovering through RAG what files are relevant to the change I'm making or discussing is very useful. Specifically I find useful that it looks up the interface of a function while I'm trying to invoke it and pass the local variables doing some light data massaging if needed.

I also like the ability of looking up documentation online and provide public API suggestions with references, I want to learn how to leverage this even more.

I really like the micro interactions and quality of life improvements. I use a lot the CMD+K and TAB-to-complete. I also keeps in memory the last few lines of code I wrote, so while I'm jumping around files the completions are relevant with the task I'm executing on e.g. it suggests to call the function that I just wrote in another module.


## What I don't like
I don't like the idea that it could one day diverge from VSCode. Philosophically the editor is such a fundamental part of my dev workflow: I spend a lot of time customizing settings and building muscle memory around keyboard shortcuts. The reason I even considered switching to Cursor is that is fully compatible with VSCode, all my extension just work perfectly.

I noticed few UI small annoyances: there was a bug with the autocompletion window being stuck on the top and not being able to move it. Since it's only sporadically rebased on top of VSCode, means it doesn't contain the latest features. At the moment of writing Cursor is using 1.93.1 from last August, while version 1.96.4 is available, so it's roughtly 5 months behind and may have impact on certain extensions in case they require the latest VSCode features.

Lastly I tried the agent mode on a pet project: I wanted to remove the unnecessary exports in a Typescript project using ts-prune and having the agent edit few files at a time. I wasn't able to keep it focused on the task, it only waits ~10s of command running before killing the terminal and when it make edits it renames modules and functions instead of just removing the `export` word. I find it quite unreliable for now, but I'm sure it's going to improve fast.

# Opportunities: what I think will improve soon
- Condescending, need more push back <video for sister Anthropic CEO>, reasoning models O1, O3 push back more
- Code generation speed
- Ecosystem: Rust error correction better than typescript because of compiler. Compiler and language server could return much reacher context 
- Editor could become a commodity once the developer community converges on a set of features, nothing in Cursor seems crazy difficult to build, they just got there fast
- Agent

Conclusions
- Code autocompletion and low intellectual stimulation BAD